RFA是一种线性时间空间attention，使用random feature methods来逼近softmax，可以作为softmax的无缝替换。

# Reference

1. [Transformer Quality in Linear Time](https://proceedings.mlr.press/v162/hua22a/hua22a.pdf)
1. [Sampled Softmax with Random Fourier Features](https://arxiv.org/pdf/1907.10747)
1. [Gated Linear Attention Transformers with Hardware-Efficient Training](https://arxiv.org/pdf/2312.06635)